{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.6 64-bit ('3.8.6': pyenv)",
   "display_name": "Python 3.8.6 64-bit ('3.8.6': pyenv)",
   "metadata": {
    "interpreter": {
     "hash": "c7f26a0e7d2773cc0bb1841732cd409189e2924866e2ae600de3c0ee1aaa95c9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "df_train = pandas.read_csv('./train.csv', quotechar=\"\\'\")\n",
    "df_test = pandas.read_csv('./test.labeled.csv', quotechar=\"\\'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading emoji data ...\n",
      "... OK (Got response in 0.44 seconds)\n",
      "Writing emoji data to /Users/bangkodir/.demoji/codes.json ...\n",
      "... OK\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import demoji\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "demoji.download_codes()\n",
    "\n",
    "def clean_tweets(data):\n",
    "    new_data = data.copy()\n",
    "\n",
    "    username_hash = r'[#@]\\w+'\n",
    "    punctuation = '[%s]+' % re.escape(string.punctuation)\n",
    "    special_char = r'[^0-9a-zA-Z\\s]+'\n",
    "    number = r'[0-9]+'\n",
    "    space = r'\\s{2,}'\n",
    "    space_begin_end = r'^\\s+|\\s+$'\n",
    "    url = r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-z]{2,4}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)'\n",
    "    char_ref = r'&\\w+;'\n",
    "\n",
    "    for i in range(len(new_data)):\n",
    "        old_str = str(new_data[i])\n",
    "\n",
    "        old_str = re.sub(username_hash, '', old_str)\n",
    "        old_str = re.sub(url, '', old_str)\n",
    "        old_str = re.sub(char_ref, ' ', old_str)\n",
    "        old_str = re.sub(punctuation, '', old_str)\n",
    "        old_str = re.sub(number, '', old_str)\n",
    "        old_str = re.sub(space_begin_end, '', old_str)\n",
    "        old_str = re.sub(space, '', old_str)\n",
    "        old_str = demoji.replace(old_str, '')\n",
    "        old_str = re.sub(special_char, '', old_str)\n",
    "\n",
    "        new_data[i] = old_str\n",
    "\n",
    "    return new_data\n",
    "\n",
    "def case_fold(data):\n",
    "    new_data = data.copy()\n",
    "    return list(map(lambda s: s.lower(), new_data))\n",
    "\n",
    "def tokenize(data):\n",
    "    new_data = data.copy()\n",
    "    return list(map(lambda s: s.split(' '), new_data))\n",
    "\n",
    "def stem(data):\n",
    "    new_data = data.copy()\n",
    "    stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "    return list(map(lambda s: stemmer.stem(s), new_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_train = clean_tweets(df_train['tweet'])\n",
    "case_folded_train = case_fold(cleaned_train)\n",
    "tweet_train = stem(case_folded_train)\n",
    "\n",
    "cleaned_test = clean_tweets(df_test['tweet'])\n",
    "case_folded_test = case_fold(cleaned_test)\n",
    "tweet_test = stem(case_folded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(use_idf=True)\n",
    "tweet_train_vect = tfidf_vect.fit_transform(tweet_train)\n",
    "tweet_test_vect = tfidf_vect.transform(tweet_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Positive Percentage : 25.0\n",
      "Negative Percentage : 75.0\n",
      "Classification Accuracy : 80.06912442396313\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svm_c = SVC(kernel='linear')\n",
    "svm_c.fit(tweet_train_vect, df_train['category'])\n",
    "\n",
    "pred = svm_c.predict(tweet_test_vect)\n",
    "\n",
    "pos = []\n",
    "neg = []\n",
    "total_raw_tweets = len(tweet_test)\n",
    "\n",
    "for i in range(total_raw_tweets):\n",
    "    tweet = tweet_test[i]\n",
    "    if pred[i] == 'positif':\n",
    "        pos.append(tweet)\n",
    "    elif pred[i] == 'negatif':\n",
    "        neg.append(tweet)\n",
    "\n",
    "pos_percent = (len(pos) / total_raw_tweets) * 100\n",
    "neg_percent = (len(neg) / total_raw_tweets) * 100\n",
    "accuracy = accuracy_score(df_test['category'], pred) * 100\n",
    "\n",
    "print(f\"Positive Percentage : {pos_percent}\")\n",
    "print(f\"Negative Percentage : {neg_percent}\")\n",
    "print(f\"Classification Accuracy : {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Kata Positif\n",
      "law - 200 kata\n",
      "omnibus - 199 kata\n",
      "uu - 118 kata\n",
      "yang - 108 kata\n",
      "luhut - 73 kata\n",
      "saya - 65 kata\n",
      "kerja - 54 kata\n",
      "ungkap - 52 kata\n",
      "di - 49 kata\n",
      "halal - 49 kata\n",
      "dan - 49 kata\n",
      "cipta - 48 kata\n",
      "bentuk - 46 kata\n",
      "awal - 44 kata\n",
      "jadi - 43 kata\n",
      "sertifikat - 38 kata\n",
      "pak - 37 kata\n",
      "ini - 36 kata\n",
      "mulai - 34 kata\n",
      "indonesia - 33 kata\n",
      "menko - 32 kata\n",
      "oleh - 31 kata\n",
      "satu - 31 kata\n",
      "ubah - 30 kata\n",
      "saat - 30 kata\n",
      "------\n",
      "Kata Negatif\n",
      "omnibus - 624 kata\n",
      "law - 618 kata\n",
      "yang - 261 kata\n",
      "uu - 231 kata\n",
      "dan - 178 kata\n",
      "di - 175 kata\n",
      "ini - 175 kata\n",
      "yg - 158 kata\n",
      "kerja - 151 kata\n",
      "ada - 148 kata\n",
      "cipta - 141 kata\n",
      "tolak - 137 kata\n",
      "tidak - 123 kata\n",
      "itu - 95 kata\n",
      "untuk - 92 kata\n",
      "aksi - 89 kata\n",
      "dari - 85 kata\n",
      "jokowi - 82 kata\n",
      "kata - 80 kata\n",
      "rakyat - 70 kata\n",
      "demo - 68 kata\n",
      "nyata - 66 kata\n",
      "ke - 64 kata\n",
      "presiden - 63 kata\n",
      "saya - 56 kata\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_top_words(data, n=25):\n",
    "    cv = CountVectorizer()\n",
    "    cv.fit(data)\n",
    "\n",
    "    bag_words = cv.transform(data)\n",
    "    sum_words = bag_words.sum(axis=0)\n",
    "\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "def print_top_words(data):\n",
    "    for word, n in data:\n",
    "        print(f\"{word} - {n} kata\")\n",
    "\n",
    "pos_top_words = get_top_words(pos)\n",
    "neg_top_words = get_top_words(neg)\n",
    "\n",
    "print(\"Kata Positif\")\n",
    "print_top_words(pos_top_words)\n",
    "print(\"------\")\n",
    "print(\"Kata Negatif\")\n",
    "print_top_words(neg_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : test our model here"
   ]
  }
 ]
}