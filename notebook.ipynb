{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.6 64-bit ('3.8.6')",
   "display_name": "Python 3.8.6 64-bit ('3.8.6')",
   "metadata": {
    "interpreter": {
     "hash": "27463e7d8f9c81c6c5e4312c7cb37a0a8f9c5b1225598cec8ab1817ebf8c64d0"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "df_train = pandas.read_csv('./50_dataset.csv', quotechar=\"\\'\")\n",
    "df_test = pandas.read_csv('./test.csv', quotechar=\"\\'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading emoji data ...\n",
      "... OK (Got response in 0.44 seconds)\n",
      "Writing emoji data to /Users/bangkodir/.demoji/codes.json ...\n",
      "... OK\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import demoji\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "demoji.download_codes()\n",
    "def clean_tweets(data):\n",
    "    new_data = data.copy()\n",
    "\n",
    "    username_hash = r'[#@]\\w+'\n",
    "    punctuation = '[%s]+' % re.escape(string.punctuation)\n",
    "    special_char = r'[^0-9a-zA-Z\\s]+'\n",
    "    number = r'[0-9]+'\n",
    "    space = r'\\s{2,}'\n",
    "    space_begin_end = r'^\\s+|\\s+$'\n",
    "    url = r'(https?|www):\\/{1,}\\w+\\W+\\w+\\/{1,}\\w+'\n",
    "    char_ref = r'&\\w+;'\n",
    "\n",
    "    for i in range(len(new_data)):\n",
    "        new_data[i] = re.sub(char_ref, ' ', new_data[i])\n",
    "        new_data[i] = re.sub(username_hash, '', new_data[i])\n",
    "        new_data[i] = re.sub(url, '', new_data[i])\n",
    "        new_data[i] = re.sub(punctuation, '', new_data[i])\n",
    "        new_data[i] = re.sub(number, '', new_data[i])\n",
    "        new_data[i] = re.sub(space_begin_end, '', new_data[i])\n",
    "        new_data[i] = re.sub(space, '', new_data[i])\n",
    "        new_data[i] = demoji.replace(new_data[i], '')\n",
    "        new_data[i] = re.sub(special_char, '', new_data[i])\n",
    "\n",
    "    return new_data\n",
    "\n",
    "def case_fold(data):\n",
    "    new_data = data.copy()\n",
    "    return list(map(lambda s: s.lower(), new_data))\n",
    "\n",
    "def tokenize(data):\n",
    "    new_data = data.copy()\n",
    "    return list(map(lambda s: s.split(' '), new_data))\n",
    "\n",
    "def stem(data):\n",
    "    new_data = data.copy()\n",
    "    stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "    return list(map(lambda s: stemmer.stem(s), new_data))\n",
    "\n",
    "def get_term(data):\n",
    "    new_data = data.copy()\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    count_vectorizer.fit(new_data)\n",
    "\n",
    "    return count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweet_train = stem(case_fold(clean_tweets(df_train['tweet'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['negatif' 'positif']\n",
      "[0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0]\n",
      "[0 1 0 0 0 0 0 1 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweet_train, df_train['category'], random_state=13)\n",
    "\n",
    "enc = LabelEncoder()\n",
    "y_train = enc.fit_transform(y_train)\n",
    "y_test = enc.fit_transform(y_test)\n",
    "\n",
    "print(enc.classes_)\n",
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'said': 392, 'iqbal': 168, 'kspi': 230, 'akan': 9, 'aksi': 10, 'besarbesaran': 62, 'desak': 106, 'dpr': 114, 'uji': 476, 'ulang': 477, 'omnibus': 329, 'law': 248, 'sampai': 398, 'hari': 142, 'ini': 161, 'kita': 214, 'bingung': 67, 'karena': 195, 'draf': 117, 'tidak': 457, 'buka': 76, 'mana': 274, 'yang': 496, 'benar': 55, 'adaversi': 3, 'bayang': 50, 'itu': 170, 'kata': 197, 'feri': 126, 'amsari': 17, 'banyak': 44, 'berita': 60, 'kutip': 235, 'polisi': 367, 'bahwa': 38, 'tangkap': 443, 'adalah': 1, 'anggur': 22, 'mungkin': 306, 'mau': 283, 'kesan': 211, 'tolak': 462, 'bukan': 77, 'siapasiapa': 420, 'padahal': 336, 'harus': 143, 'baca': 34, 'begini': 52, 'saja': 393, 'percaya': 353, 'uu': 485, 'cipta': 89, 'kerja': 207, 'untuk': 480, 'mereka': 291, 'juta': 185, 'rakyat': 376, 'indonesia': 158, 'yg': 497, 'nonton': 320, 'mata': 282, 'najwa': 309, 'malam': 272, 'jadi': 171, 'saksi': 394, 'anggota': 21, 'dari': 97, 'pdip': 350, 'kena': 205, 'proses': 372, 'lahir': 240, 'kalau': 188, 'ada': 0, 'cacatcacat': 81, 'sedikit': 404, 'arti': 27, 'valid': 487, 'yah': 495, 'bentuk': 57, 'tuh': 465, 'uucacat': 486, 'terimakasih': 454, 'politis': 368, 'telah': 448, 'aku': 12, 'ikut': 153, 'atur': 31, 'telekomunikasi': 449, 'apa': 23, 'dampak': 95, 'industri': 160, 'di': 108, 'indonesiavia': 159, 'ya': 494, 'lagi': 237, 'susah': 436, 'gin': 134, 'ketok': 213, 'palu': 340, 'sahin': 391, 'ayo': 33, 'lihat': 257, 'kasus': 196, 'kecil': 203, 'seperti': 413, 'tahan': 439, 'syahganda': 438, 'maksud': 270, 'borgol': 71, 'bilang': 66, 'intai': 163, 'kontra': 220, 'boleh': 69, 'pihak': 363, 'dan': 96, 'presiden': 369, 'diam': 109, 'soal': 427, 'mardani': 277, 'ali': 14, 'sera': 414, 'mengapa': 288, 'hrs': 149, 'berulangkali': 61, 'tanya': 444, 'hardik': 141, 'asfin': 29, 'ylbhi': 498, 'ilc': 154, 'apasiapa': 25, 'untung': 481, 'jg': 178, 'wakil': 490, 'pikir': 364, 'rakyatpendemoakibatnya': 377, 'respektrust': 384, 'pd': 349, 'kuasa': 233, 'makin': 269, 'merosot': 292, 'pasti': 347, 'rasa': 379, 'adil': 4, 'rampok': 378, 'faham': 124, 'luhut': 264, 'kan': 194, 'tokoh': 460, 'balik': 41, 'bos': 72, 'baru': 47, 'bank': 43, 'mandiri': 275, 'seru': 417, 'solidaritas': 428, 'mari': 278, 'gabung': 129, 'sama': 396, 'kami': 192, 'lanjut': 246, 'bangkang': 42, 'sipil': 423, 'hadap': 138, 'dengan': 104, 'turun': 471, 'ke': 200, 'jalan': 172, 'lagioktobergerbang': 238, 'tol': 461, 'cileunyi': 88, 'sah': 390, 'buah': 74, 'pabrik': 335, 'otomotif': 333, 'nama': 311, 'langsung': 245, 'phkpekerja': 362, 'isu': 169, 'utk': 484, 'gelombang': 132, 'pertama': 358, 'krn': 229, 'covid': 92, 'tunggu': 469, 'aja': 6, 'bentar': 56, 'ganti': 131, 'kontrak': 221, 'outsourcing': 334, 'semua': 410, 'selamat': 405, 'datang': 99, 'era': 123, 'dikerjain': 110, 'moeldoko': 298, 'sebut': 403, 'bakal': 40, 'nikmat': 319, 'calon': 82, 'innalillahi': 162, 'bem': 54, 'unidayan': 479, 'baubau': 48, 'duga': 121, 'tembak': 450, 'dalam': 94, 'demo': 102, 'ungkap': 478, 'awal': 32, 'saya': 401, 'mulaiplus': 304, 'pak': 337, 'mahfud': 267, 'jimly': 179, 'and': 20, 'indiarto': 157, 'seno': 411, 'aji': 8, 'gubernur': 137, 'malu': 273, 'utara': 483, 'surati': 435, 'jokowi': 181, 'mulaiwell': 305, 'noted': 321, 'opung': 331, 'asfinawati': 30, 'srikandi': 429, 'sumitomo': 434, 'menyusulinvestor': 290, 'global': 135, 'belum': 53, 'kekuatiran': 204, 'tentang': 452, 'tarik': 445, 'investor': 167, 'kualitas': 232, 'obral': 323, 'murah': 308, 'negeri': 317, 'kepada': 206, 'kalengkaleng': 189, 'turut': 472, 'salah': 395, 'ud': 474, 'was': 493, 'naskah': 313, 'gak': 130, 'jelas': 176, 'takut': 441, 'kritik': 228, 'pasal': 346, 'demi': 101, 'nyata': 322, 'jebak': 175, 'betmen': 63, 'panjang': 341, 'lebar': 252, 'kurang': 234, 'beda': 51, 'versi': 488, 'cilaka': 87, 'nanti': 312, 'sereem': 415, 'cetus': 86, 'ide': 151, 'jujur': 182, 'mulai': 302, 'berat': 58, 'lbp': 251, 'lah': 239, 'biang': 64, 'rusuh': 387, 'lama': 244, 'sangat': 399, 'kait': 186, 'penting': 352, 'manusia': 276, 'satu': 400, 'oktoberaksi': 326, 'paramedis': 345, 'alhamdulillah': 13, 'korban': 223, 'sudah': 432, 'tau': 447, 'oknum': 325, 'tunggang': 468, 'serta': 416, 'sebar': 402, 'hoax': 148, 'provokasi': 373, 'mahasiswa': 266, 'buruh': 79, 'hingga': 146, 'ajar': 7, 'paksa': 339, 'laku': 242, 'tindak': 458, 'anarkis': 19, 'rusak': 386, 'petinggi': 361, 'negaranegara': 315, 'asean': 28, 'kecam': 202, 'kesah': 210, 'perintah': 354, 'promosi': 371, 'ciptaker': 90, 'usaha': 482, 'jepang': 177, 'lalu': 243, 'tri': 463, 'luarnegri': 262, 'hendra': 144, 'siregar': 424, 'cekal': 85, 'kovid': 227, 'bongkar': 70, 'tuntas': 470, 'pks': 365, 'klaim': 215, 'temu': 451, 'selundup': 407, 'final': 127, 'ohh': 324, 'pantas': 342, 'laju': 241, 'kayak': 199, 'bus': 80, 'patas': 348, 'ternate': 455, 'wartawan': 492, 'halang': 139, 'ketika': 212, 'liput': 259, 'aman': 15, 'saat': 389, 'lawsalah': 249, 'bahkan': 37, 'teriak': 453, 'ngoni': 318, 'kalian': 191, 'pakai': 338, 'hallo': 140, 'live': 260, 'report': 382, 'pukulwib': 375, 'sembari': 409, 'mungut': 307, 'sampah': 397, 'oleh': 327, 'massa': 280, 'kontras': 222, 'wajar': 489, 'represif': 383, 'aparat': 24, 'masyarakat': 281, 'may': 284, 'dayaksi': 100, 'bawaslumeireformasi': 49, 'dikorupsiseptemberhingga': 111, 'closing': 91, 'statement': 430, 'rocky': 385, 'gerung': 133, 'najwaomnibus': 310, 'pesan': 360, 'para': 344, 'plutokrat': 366, 'orang': 332, 'kaya': 198, 'ijon': 152, 'beri': 59, 'imbal': 155, 'muka': 301, 'kleptokrat': 216, 'korup': 224, 'toilet': 459, 'paper': 343, 'depan': 105, 'md': 286, 'bagai': 35, 'makan': 268, 'lezat': 256, 'ludah': 263, 'migas': 294, 'cantum': 83, 'meski': 293, 'tak': 440, 'pernah': 355, 'tuju': 466, 'ahli': 5, 'hukum': 150, 'tata': 446, 'negaraxaugmzainal': 316, 'arifin': 26, 'mochtarxakembali': 297, 'mengkritisixauu': 289, 'kerjaxayang': 209, 'masih': 279, 'tuai': 464, 'pro': 370, 'jumlah': 183, 'kalang': 187, 'kali': 190, 'zainal': 499, 'mochtar': 296, 'buat': 75, 'ugalugalan': 475, 'hidup': 145, 'miskin': 295, 'kota': 226, 'maaf': 265, 'klo': 218, 'sy': 437, 'sll': 426, 'penasaran': 351, 'brp': 73, 'persen': 356, 'sih': 421, 'dr': 116, 'klmpk': 217, 'ktnya': 231, 'dgn': 107, 'cara': 84, 'mrk': 299, 'jangan': 173, 'hny': 147, 'suara': 431, 'aktivis': 11, 'kampus': 193, 'negara': 314, 'amerika': 16, 'federal': 125, 'indeks': 156, 'persepsi': 357, 'korupsi': 225, 'jauh': 174, 'lebih': 253, 'baik': 39, 'drpd': 118, 'dua': 119, 'demokrasi': 103, 'duduk': 120, 'relative': 381, 'sentralistik': 412, 'skrg': 425, 'malah': 271, 'bikin': 65, 'bisa': 68, 'intervensi': 164, 'jlebstatement': 180, 'mbak': 285, 'direktur': 112, 'memang': 287, 'ruu': 388, 'lindung': 258, 'prt': 374, 'selamatahunruu': 406, 'adat': 2, 'bahas': 36, 'kebut': 201, 'cuma': 93, 'siapa': 419, 'logika': 261, 'mudah': 300, 'investasimembuka': 166, 'lap': 247, 'kerjamemulihkan': 208, 'ekonomi': 122, 'datafakta': 98, 'sulit': 433, 'bukti': 78, 'justru': 184, 'legitimasi': 254, 'konsentrasi': 219, 'uang': 473, 'investasi': 165, 'tangan': 442, 'oligark': 328, 'analisis': 18, 'lbh': 250, 'lengkap': 255, 'tulis': 467, 'sini': 422, 'dis': 113, 'oohhh': 330, 'si': 418, 'bapak': 45, 'perusahaantambangnya': 359, 'mulaikeluarin': 303, 'warga': 491, 'kwitang': 236, 'semangat': 408, 'terus': 456, 'bara': 46, 'rdp': 380, 'dprd': 115, 'fspmi': 128, 'gresik': 136}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer()\n",
    "vect.fit(tweets)\n",
    "\n",
    "X_train_tfidf = vect.transform(X_train)\n",
    "X_test_tfidf = vect.transform(X_test)\n",
    "\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.7692307692307693\n",
      "positive:  {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 3}\n",
      "negative:  {'precision': 0.7692307692307693, 'recall': 1.0, 'f1-score': 0.8695652173913044, 'support': 10}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svm_c = SVC(kernel='linear')\n",
    "svm_c.fit(X_train_tfidf, y_train)\n",
    "pred = svm_c.predict(X_test_tfidf)\n",
    "\n",
    "print(accuracy_score(pred, y_test))\n",
    "\n",
    "report = classification_report(y_test, pred, output_dict=True)\n",
    "print('positive: ', report['1'])\n",
    "print('negative: ', report['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}